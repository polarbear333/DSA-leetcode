Garbage collection is an **automatic memory management** technique where the system reclaims memory occupied by objects that are no longer in use, preventing memory leaks. It relieves the programmer from doing **manual memory management**, where the programming specifies what objects to do de-allocate and return to the memory system and when to do so. Similar **automatic memory management** techniques include:
- Stack allocation
- Region inference
- Memory ownership

**Advantages:** GC frees programmers from manually deallocating the memory, to prevent some errors:
- **Dangling pointers**, which occur when a piece of memory is freed but there are still pointers to it, and one of those pointers is dereferenced. By then the memory may have been reassigned to another use, with unpredictable results.
- **Double free bugs**, which occur when the program tries to free a region of memory that has already been freed, and perhaps already been allocated again.
- Certain kind of **memory leaks**, in which a program fails to free memory occupied by objects that have become unreachable, causing memory exhaustion 

**Disadvantages**: Garbage collection may take a significant proportion of a program's total processing time, and affect performance as a result. The reason is because it consumes the **CPU cycles** to execute the garbage collector's code, that could otherwise be used to run the application's code. The work includes:
- **Identifying live objects:** Determining which objects in memory are still being referenced by the program.
- **Identifying dead objects:** Identifying objects that are no longer referenced and are candidates for collection.
- **Reclaiming memory:** Freeing up the memory occupied by dead objects.
- **Potentially compacting memory:** Moving live objects around to reduce fragmentation and improve allocation speed (in some GC algorithms).

As GC uses computer resources to decide which memory to free, the penalty for the convenience of not annotating object lifetime manually in the source code is **overhead**, which impairs system performance. A peer-reviewed paper from 2005 concluded that GC needs five times the memory to compensate for this overhead and to perform as fast as the same program using idealized explicit memory management.

The comparison however is made to a program generated by deallocation calls using a **oracle**, implementing by collecting traces from programs run under a profiler, and the program is only correct for one particular execution of the program.  Interaction with memory hierarchy effects can make this overhead intolerable in circumstances that are hard to predict or to detect in routine testing.

Resources other than memory, such as network sockets, database handles, windows, file descriptors, and device descriptors, are not typically handled by garbage collection, but rather by other methods (e.g. destructors). Some such methods de-allocate memory also.

### **Common Algorithms / Strategies** :

 **Tracing:** Most commonly seen GC method, overall strategy consists of determining which objects should be garbage collected by tracing which objects are _reachable_ by a chain of references from certain root objects, and considering the rest as garbage and collecting them.

**Reachability of an object:** Informally, an object is reachable if it is referenced by at least one variable in the program, either directly or through references from other reachable objects: 
1. **A distinguished set of roots:** objects that are assumed to be reachable. Typically these include all the objects referenced anywhere from the call stack (local variables, parameters in the functions being invoked), and any global variables.
2. Anything referenced from a reachable object itself is reachable, more formally, reachability is a **transitive closure**.

The reachability definition of "garbage" is not optimal, insofar as the last time a program uses an object could be long before that object falls out of the environment scope. A distinction is sometimes drawn between **syntactic garbage**, those objects the program cannot possibly reach, and **semantic garbage** , those objects the program will in fact never again use. For example:

```cpp
Object x = new Foo();
Object y = new Bar();
x = new Quux();

/* At this point, we know that the Foo object 
 * originally assigned to x will never be
 * accessed: it is syntactic garbage.
 */

/* In the following block, y *could* be semantic garbage;
 * but we won't know until x.check_something() returns
 * some value -- if it returns at all.
 */
 
if (x.check_something()) {
    x.do_something(y);
}
System.exit(0);
```

The problem of precisely identifying semantic garbage can easily be shown to be partially decidable: a program that allocates an object ${\displaystyle X}$ , runs an arbitrary input program ${\displaystyle P}$, and uses ${\displaystyle X}$ if and only if ${\displaystyle P}$ finishes would require a semantic garbage collector to solve the halting problem. Although conservative heuristic methods for semantic garbage detection remain an active research area, essentially all practical garbage collectors focus on syntactic garbage.

**Strong and weak references:** The garbage collector can reclaim only objects that have no references pointing to them either directly or indirectly from the root set. However, some programs require **weak references**, which should be usable for as long as the object exists but should not prolong its lifetime. In discussions about weak references, ordinary references are sometimes called **strong references**. An object is eligible for garbage collection if there are no strong (i.e ordinary) references to it, even though there might be some weak references to it.

A weak reference is not merely just any pointer to the object that a garbage collector does not care about. The term is usually reserved for properly managed category of special reference objects which are safe to use even after the object disappears because they _lapse_ to a safe value (usually `null`). An unsafe reference that is not known to the garbage collector will simply remain dangling by continuing to refer to the address where the object previously resided.

**Tracing Algorithms:** Tracing collectors are so called because they trace through the working set of memory, and performs collection in cycles. It is common for cycles to be triggered when there is not enough free memory for the memory manager to satisfy an allocation request, but cycles can often be requested by the mutator directly or run on a time schedule.
- **Mark and Sweep**: Traverse object graph → Mark reachable → Sweep unmarked. In the naive mark-and-sweep method, each object in memory has a flag (typically a single bit) reserved for garbage collection use only. This flag is always _cleared_, except during the collection cycle.

  1. **Mark Phase:** When an object is created, its mark bit is set to 0 (false bit). In the Mark phase, we set the marked bit for all the reachable objects (or the objects which a user can refer to) to 1(true). We would do a graph traversal for this operation, like DFS would work for us. Here we can consider every object as a node and then all the nodes (objects) that are reachable from this node (object) are visited and it goes on till we have visited all the reachable nodes. 
		- The root is a variable that refers to an object and is directly accessible 
		 by a local variable.  
		- We can access the mark bit for an object by `markedBit(obj)`.
**Algorithm**: Mark phase
```
 Mark(root)
 If markedBit(root) = false then
					 markedBit(root) = true
										For each v
 referenced by root
										 Mark(v)
```

2. **Sweep Phase**: After mark phase, we would sweep all the unreachable objects (i.e it clears the heap memory for all the unreachable objects.) All those objects whose marked value is set to false are cleared from the heap memory, for all other objects (reachable objects) the marked bit is set to true. Now the mark value for all the reachable objects is set to false since we will run the algorithm (if required) and again we will go through the mark phase to mark all the reachable objects. 
**Algorithm**: Sweep phase
```
Sweep()
For each object p in heap
If markedBit(p) = true then
				markedBit(p) = false
								else
									heap.release(p)
```
	
**Advantages**: 
- It handles the case with cyclic references, even in the case of a cycle, this algorithm never ends in an infinite loop
- No additional overheads incurred during execution

**Disadvantages**: 
- Normal program execution is suspended while the garbage collection runs (consumed by CPU cycles to run GC
- After the algorithm is being run several times, reachable objects end up being separated by many, small unused memory regions.
``` Heap memory
[].[ ]..[  ].[ ]...[]
```
Here the dots represents the free memory, while the bracket blocks represent the memory taken by all the reachable objects.

Now the free segments (dots) are of varying sizes let's say the 5 free segments are of size 1, 1, 2, 3, 5(size in units). Now we need to create an object which takes 10 units of memory, now assuming that memory can be allocated only in the **contiguous** form on blocks, the creation of an object is not possible although we have an available memory space of 12 units, and it will cause **OutOfMemory** error.

This problem is termed **Fragmentation**. We have memory available in "fragments" but we are unable to utilize that memory space. We can reduce the fragmentation by compaction; we shuffle the memory content to place all the memory blocks together to form one large block. Now consider the above example, after compaction we have a contiguous block of free memory size 12 units, so we can now allocate memory to an object size of 10 units.

**Variants of mark-and-sweep**: 

**Serial GC**: A simple, single-threaded mark-and-sweep GC that operates in a stop-the-world manner, which pauses the application's execution during garbage collection, and also does compaction to reduce fragmentation after. It uses the same logic with mark-and sweep, with a single thread approach:
- **Mark Phase**: Single thread traverses roots and marks all objects directly referenced by the application’s active threads to identify all reachable objects.
- **Sweep Phase**: Once the marking is complete, the algorithm single threadly sweeps the memory regions, deallocating memory occupied by unreferenced objects. 
- **Compaction**:  Compacts the memory by moving live objects together after sweep, creating a contiguous free memory space.
- **Free Memory**: After compaction, the algorithm updates the free memory space pointer to indicate the new location for object allocation.

**Pros:** 
1. **Simplicity**: The Serial GC is straightforward to implement and understand, making it an ideal choice for beginners or smaller applications.
2. **Low Overhead**: The algorithm has lower CPU and memory overhead compared to more complex GC algorithms, making it suitable for resource-constrained environments.
3. **Predictable Pauses**: As a stop-the-world collector, the Serial GC provides predictable pauses during garbage collection, allowing for better control over the application behavior.
4. **Single-threaded Execution:** The Serial GC performs garbage collection using a single thread, ensuring **determinism** and avoiding potential multi-threading issues.

**Cons**:
1. **Longer Pause Times:**  The stop-the-world nature of the Serial GC can result in longer pause times, causing interruptions in application responsiveness for larger heaps or memory-intensive applications.
2. **Limited Scalability:**  The single-threaded nature of the Serial GC limits its scalability on modern multi-core processors, where parallelism can significantly improve GC performance.
3. **Not Suitable for Large Applications:** The Serial GC may not be the optimal choice for large-scale applications or systems with high memory requirements, as its performance may degrade due to increased garbage collection times.

**Parallel GC:** Also uses mark-and-sweep, but taking advantage of multi-core processors and improving GC performance by parallelizing certain tasks, such that the parallel GC threads make collection faster, and wait lesser time during Stop-the-World period.
- **Mark phase**: Similar to Serial GC, the algorithm identifies and marks all objects directly referenced by the application's active threads. This marking process occurs while the application is paused momentarily. 
- **Sweep phase**: 





- **Generational GC**: Divide heap into young (new objects) and old generations (long-lived objects). New objects are collected more often (cheaper).
- **Reference Counting**: Objects are deallocated when their reference count drops to zero (issue: circular references).
- **Concurrent Collectors**: e.g., **G1 GC** in Java, minimize pause times by doing most work concurrently with application threads.